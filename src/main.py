import time
import random
from pathlib import Path

import streamlit as st
import torch
import numpy as np
from model import load_model_from_hf
from utils import get_prompt
from vllm import SamplingParams


BASE_DIR = Path(__file__).resolve().parent

st.set_page_config(page_title="LLM Prompt-Playground", page_icon="ğŸ¤¹")
st.header("Playground for prompt engineering", anchor="top", divider="rainbow")

# ì‹œë“œ ê³ ì • í•¨ìˆ˜
def seed_everything(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)

sampleparams=SamplingParams().update_from_generation_config({
  "max_tokens": 512,
  "stop_token_ids": 128001,
  "do_sample": True,
  "temperature": 0.3,
  "top_p": 0.9
    }
)

# UI ëª¨ë¸ ì„¤ì •
model_path = st.sidebar.text_input("Enter Hugging Face or Local Model Path (e.g., 'gpt2')", value="")
system_prompt = st.sidebar.text_area("System Prompt", value="This is the default system prompt.")

# ëŒ€í™” ë¡œê·¸ ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
if st.sidebar.button("Reset Chat Log"):
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

# ëª¨ë¸ ë¡œë”©
if "llm_model" not in st.session_state:
    st.session_state.llm_model = None

# ìƒˆë¡œìš´ ëª¨ë¸ ë¡œë”© (ìƒˆë¡œìš´ ëª¨ë¸ ê²½ë¡œ ì…ë ¥ ì‹œ)
if model_path and model_path != getattr(st.session_state, "last_model_path", None):
    # ìƒˆë¡œìš´ ëª¨ë¸ì„ ë¡œë“œí•  ë•Œë§Œ
    st.session_state.llm_model, tokenizer = load_model_from_hf(model_path, existing_model=st.session_state.llm_model)
    st.session_state.last_model_path = model_path  # ì…ë ¥ëœ ëª¨ë¸ ê²½ë¡œë¥¼ ê¸°ë¡


# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    if message["role"] != 'system':
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("Input yout message."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ëª¨ë¸ ì‘ë‹µ ì²˜ë¦¬
    if st.session_state.llm_model:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                time.sleep(1)

            # system promptì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_text = get_prompt(st.session_state.messages)

            # ëª¨ë¸ ì‘ë‹µ ìƒì„±
            response = st.session_state.llm_model.generate(prompt_text,
                                                           sampling_params=sampleparams
                                                           )
            
            full_response = response[0].outputs[0].text.replace(prompt_text, "").strip()
            message_placeholder.markdown(full_response)
        
        # ëª¨ë¸ ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": full_response})
